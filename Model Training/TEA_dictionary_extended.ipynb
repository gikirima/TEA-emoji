{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# LANGKAH 0: INSTALASI DAN IMPORT LIBRARY\n",
        "\n",
        "## Menginstal library yang diperlukan dari Hugging Face dan Scikit-learn."
      ],
      "metadata": {
        "id": "ChXSSNUAUCtO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers datasets scikit-learn scikit-multilearn emoji torch --quiet"
      ],
      "metadata": {
        "id": "fnCHSC_nUDX1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "import emoji\n",
        "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments\n",
        "from datasets import Dataset"
      ],
      "metadata": {
        "id": "6_uioX-2UHns"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# LANGKAH 1: MEMUAT DATASET"
      ],
      "metadata": {
        "id": "wy3DXbgAUJat"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "    df = pd.read_csv('https://raw.githubusercontent.com/gikirima/TEA-emoji/refs/heads/main/dataset_emoji_only.csv')\n",
        "    print(\"Dataset berhasil dimuat.\")\n",
        "\n",
        "except FileNotFoundError:\n",
        "    print(\"Error: File tidak ditemukan.\")\n",
        "    print(\"Silakan unggah file tersebut terlebih dahulu.\")\n",
        "    # Hentikan eksekusi jika file tidak ada\n",
        "    exit()"
      ],
      "metadata": {
        "id": "GEIdGWjVUM9F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# LANGKAH 2: EKSTRAKSI EMOJI (Sama seperti di Untitled11.ipynb)\n",
        "\n",
        "## Langkah ini penting untuk memastikan semua emoji dari dataset Anda dikenali."
      ],
      "metadata": {
        "id": "zyRBgFh5UXLN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\nMemindai dataset untuk mencari emoji unik...\")\n",
        "unique_emojis = set()\n",
        "for text in df['text'].dropna(): # Menggunakan dropna() untuk menghindari error pada data kosong\n",
        "    text_str = str(text)\n",
        "    for character in text_str:\n",
        "        if emoji.is_emoji(character):\n",
        "            unique_emojis.add(character)\n",
        "\n",
        "emoji_list_to_add = list(unique_emojis)\n",
        "print(f\"Ditemukan {len(emoji_list_to_add)} emoji unik untuk ditambahkan ke tokenizer.\")"
      ],
      "metadata": {
        "id": "i_pjQwtzUc7V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# LANGKAH 3: MEMUAT TOKENIZER DAN MODEL (Integrasi dari Untitled11.ipynb)\n",
        "\n",
        "## Kita memuat model dan tokenizer dasar, lalu langsung menambahkan token emoji dan menyesuaikan ukuran embedding model."
      ],
      "metadata": {
        "id": "Xs3lwiPIUeLN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = \"indobenchmark/indobert-base-p1\"\n",
        "\n",
        "# Tentukan kolom label Anda. Sesuaikan jika nama kolom di CSV Anda berbeda.\n",
        "label_columns = ['joy', 'trust', 'fear', 'surprise', 'sadness', 'disgust', 'anger', 'anticipation']\n",
        "\n",
        "num_labels = len(label_columns)\n",
        "\n",
        "print(f\"\\nMemuat tokenizer '{model_name}'...\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "print(f\"Memuat model '{model_name}' untuk klasifikasi multi-label dengan {num_labels} label...\")\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\n",
        "    model_name,\n",
        "    num_labels=num_labels,\n",
        "    problem_type=\"multi_label_classification\" # Penting untuk klasifikasi multi-label\n",
        ")\n",
        "\n",
        "\n",
        "# --- Proses penambahan emoji dari notebook sebelumnya ---\n",
        "print(f\"Menambahkan {len(emoji_list_to_add)} token emoji ke tokenizer...\")\n",
        "tokenizer.add_tokens(emoji_list_to_add)\n",
        "print(\"Menyesuaikan ukuran token embedding pada model...\")\n",
        "model.resize_token_embeddings(len(tokenizer))\n",
        "# ----------------------------------------------------\n",
        "\n",
        "print(\"Tokenizer dan model siap digunakan dengan vocabulary emoji yang diperbarui.\")"
      ],
      "metadata": {
        "id": "Fjax_T_SUneV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# LANGKAH 4: PERSIAPAN DATASET UNTUK HUGGING FACE\n",
        "\n",
        "## Mengubah format dari Pandas DataFrame menjadi Hugging Face Dataset."
      ],
      "metadata": {
        "id": "DaIGJMg9UpIc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Pastikan Anda sudah menginstal scikit-multilearn\n",
        "# !pip install scikit-multilearn --quiet\n",
        "from skmultilearn.model_selection import iterative_train_test_split\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "print(\"\\nMemulai pembagian dataset dengan metode iterative_train_test_split (PERBAIKAN)...\")\n",
        "\n",
        "# Definisikan kolom fitur (X) dan label (y)\n",
        "label_columns = ['joy', 'trust', 'fear', 'surprise', 'sadness', 'disgust', 'anger', 'anticipation']\n",
        "\n",
        "# Penting: Konversi kolom teks dan label ke format numpy untuk splitter\n",
        "X = df['text'].to_numpy().reshape(-1, 1)\n",
        "y = df[label_columns].to_numpy()\n",
        "\n",
        "\n",
        "# --- Pembagian Data yang Benar ---\n",
        "\n",
        "# 1. Pisahkan 20% data untuk set tes (unseen data)\n",
        "# Fungsi ini mengembalikan X dan y untuk masing-masing set\n",
        "X_train_val, y_train_val, X_test, y_test = iterative_train_test_split(X, y, test_size=0.2)\n",
        "\n",
        "# 2. Pisahkan sisa 80% (train_val) menjadi data latih (train) dan validasi (validation)\n",
        "# Proporsi untuk validasi dari sisa data adalah 0.16 / 0.80 = 0.2\n",
        "X_train, y_train, X_val, y_val = iterative_train_test_split(X_train_val, y_train_val, test_size=0.2)\n",
        "\n",
        "# --- Rekonstruksi DataFrame dari hasil split ---\n",
        "# Kita perlu mengubah kembali hasil split (numpy array) menjadi Pandas DataFrame\n",
        "# untuk digunakan oleh library 'datasets'\n",
        "\n",
        "# Membuat DataFrame train\n",
        "df_train = pd.DataFrame(X_train.flatten(), columns=['text'])\n",
        "for i, col in enumerate(label_columns):\n",
        "    df_train[col] = y_train[:, i]\n",
        "\n",
        "# Membuat DataFrame validasi\n",
        "df_val = pd.DataFrame(X_val.flatten(), columns=['text'])\n",
        "for i, col in enumerate(label_columns):\n",
        "    df_val[col] = y_val[:, i]\n",
        "\n",
        "# Membuat DataFrame test\n",
        "df_test = pd.DataFrame(X_test.flatten(), columns=['text'])\n",
        "for i, col in enumerate(label_columns):\n",
        "    df_test[col] = y_test[:, i]\n",
        "\n",
        "# --- Konversi ke Hugging Face Dataset ---\n",
        "print(\"\\nMengubah format DataFrame menjadi Hugging Face Dataset...\")\n",
        "train_dataset_hf = Dataset.from_pandas(df_train)\n",
        "eval_dataset_hf = Dataset.from_pandas(df_val)\n",
        "test_dataset_hf = Dataset.from_pandas(df_test)\n",
        "\n",
        "# --- Tokenisasi dan Pemrosesan (diterapkan ke setiap dataset) ---\n",
        "# Fungsi untuk tokenisasi teks\n",
        "def tokenize_function(examples):\n",
        "    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True, max_length=128)\n",
        "\n",
        "# Fungsi untuk memformat kolom label\n",
        "def format_labels(example):\n",
        "    example[\"labels\"] = [float(example[label]) for label in label_columns]\n",
        "    return example\n",
        "\n",
        "print(\"\\nMelakukan tokenisasi dan pemformatan label pada semua set data...\")\n",
        "train_dataset = train_dataset_hf.map(tokenize_function, batched=True).map(format_labels)\n",
        "eval_dataset = eval_dataset_hf.map(tokenize_function, batched=True).map(format_labels)\n",
        "test_dataset = test_dataset_hf.map(tokenize_function, batched=True).map(format_labels)\n",
        "\n",
        "\n",
        "print(\"\\n--- Pembagian Data Selesai (Hasil Seharusnya Benar) ---\")\n",
        "print(f\"Jumlah data latih (train): {len(train_dataset)} (Target: ~64%)\")\n",
        "print(f\"Jumlah data validasi (validation): {len(eval_dataset)} (Target: ~16%)\")\n",
        "print(f\"Jumlah data uji (test): {len(test_dataset)} (Target: ~20%)\")\n",
        "print(f\"Total: {len(train_dataset) + len(eval_dataset) + len(test_dataset)}\")\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "\n",
        "print(\"\\nMenghitung dan menampilkan distribusi label di setiap set data...\")\n",
        "\n",
        "def get_label_distribution(dataset_hf, label_columns):\n",
        "    \"\"\"Menghitung jumlah kemunculan setiap label dalam dataset Hugging Face.\"\"\"\n",
        "    df = dataset_hf.to_pandas()\n",
        "    distribution = df[label_columns].sum().sort_values(ascending=False)\n",
        "    return distribution\n",
        "\n",
        "# Hitung distribusi untuk setiap set\n",
        "train_label_dist = get_label_distribution(train_dataset_hf, label_columns)\n",
        "eval_label_dist = get_label_distribution(eval_dataset_hf, label_columns)\n",
        "test_label_dist = get_label_distribution(test_dataset_hf, label_columns)\n",
        "\n",
        "# Buat DataFrame gabungan untuk plotting\n",
        "dist_df = pd.DataFrame({\n",
        "    'Train': train_label_dist,\n",
        "    'Validation': eval_label_dist,\n",
        "    'Test': test_label_dist\n",
        "})\n",
        "\n",
        "# Plotting\n",
        "fig, axes = plt.subplots(1, 3, figsize=(18, 6), sharey=True)\n",
        "fig.suptitle('Distribusi Label di Dataset Train, Validation, dan Test', fontsize=16)\n",
        "\n",
        "# Plot Train\n",
        "sns.barplot(x=dist_df.index, y='Train', data=dist_df, ax=axes[0], palette='viridis')\n",
        "axes[0].set_title('Train Dataset')\n",
        "axes[0].set_ylabel('Jumlah Kemunculan Label')\n",
        "axes[0].tick_params(axis='x', rotation=45)\n",
        "\n",
        "# Plot Validation\n",
        "sns.barplot(x=dist_df.index, y='Validation', data=dist_df, ax=axes[1], palette='viridis')\n",
        "axes[1].set_title('Validation Dataset')\n",
        "axes[1].set_ylabel('') # Hide y-label as it's shared\n",
        "axes[1].tick_params(axis='x', rotation=45)\n",
        "\n",
        "# Plot Test\n",
        "sns.barplot(x=dist_df.index, y='Test', data=dist_df, ax=axes[2], palette='viridis')\n",
        "axes[2].set_title('Test Dataset')\n",
        "axes[2].set_ylabel('') # Hide y-label as it's shared\n",
        "axes[2].tick_params(axis='x', rotation=45)\n",
        "\n",
        "plt.tight_layout(rect=[0, 0.03, 1, 0.95]) # Adjust layout to prevent title overlap\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nDistribusi label berhasil ditampilkan.\")"
      ],
      "metadata": {
        "id": "UPIrPg51VOLF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# LANGKAH 5: MENDEFINISIKAN METRIK EVALUASI\n",
        "## Fungsi ini akan menghitung metrik performa model saat evaluasi."
      ],
      "metadata": {
        "id": "nT2jau2qVPbF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_metrics(p):\n",
        "    logits, labels = p\n",
        "    # Menggunakan sigmoid karena ini adalah klasifikasi multi-label\n",
        "    preds_proba = torch.sigmoid(torch.tensor(logits))\n",
        "    # Threshold 0.5 untuk menentukan apakah label aktif (1) atau tidak (0)\n",
        "    preds = (preds_proba > 0.5).int().numpy()\n",
        "    labels = labels.astype(int)\n",
        "\n",
        "    f1_micro = f1_score(labels, preds, average='micro')\n",
        "    f1_macro = f1_score(labels, preds, average='macro')\n",
        "    precision_micro = precision_score(labels, preds, average='micro')\n",
        "    recall_micro = recall_score(labels, preds, average='micro')\n",
        "\n",
        "    return {\n",
        "        'f1_macro': f1_macro,\n",
        "        'f1_micro': f1_micro,\n",
        "        'precision_micro': precision_micro,\n",
        "        'recall_micro': recall_micro,\n",
        "        'accuracy': accuracy_score(labels, preds), # Accuracy berbasis exact match\n",
        "    }"
      ],
      "metadata": {
        "id": "djF3VAInVWlt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# LANGKAH 6: KONFIGURASI DAN PROSES TRAINING\n",
        "\n",
        "## Menentukan argumen pelatihan dan memulai proses fine-tuning."
      ],
      "metadata": {
        "id": "pphKYCvuVXhk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import wandb\n",
        "from google.colab import userdata\n",
        "\n",
        "# Login to WANDB\n",
        "# Make sure you have added your WANDB API key to Colab secrets with the name WANDB_API_KEY\n",
        "try:\n",
        "    wandb_api_key = userdata.get('WANDB_API_KEY')\n",
        "    if wandb_api_key:\n",
        "        wandb.login(key=wandb_api_key)\n",
        "        print(\"WANDB login successful.\")\n",
        "    else:\n",
        "        print(\"WANDB_API_KEY not found in Colab secrets. Please add it to use WANDB logging.\")\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred during WANDB login: {e}\")\n",
        "\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    # --- Direktori & Strategi ---\n",
        "    output_dir=\"./results\",\n",
        "    eval_strategy=\"steps\",          # Evaluasi per step\n",
        "    save_strategy=\"steps\",          # Simpan checkpoint per step\n",
        "    load_best_model_at_end=True,   # Muat model terbaik di akhir\n",
        "    metric_for_best_model=\"f1_macro\",\n",
        "    greater_is_better=True,         # f1_macro yang lebih tinggi lebih baik\n",
        "\n",
        "    # --- Frekuensi Step ---\n",
        "    logging_steps=250,             # Catat log setiap 250 steps\n",
        "    eval_steps=1500,         # Evaluasi setiap 1500 steps\n",
        "    save_steps=1500,               # Simpan checkpoint setiap 1500 steps\n",
        "    save_total_limit=3,            # Simpan maksimal 3 checkpoint (termasuk yg terbaik)\n",
        "\n",
        "    # --- Hyperparameter Training ---\n",
        "    per_device_train_batch_size=16,\n",
        "    per_device_eval_batch_size=16,\n",
        "    num_train_epochs=3,\n",
        "    learning_rate=3e-5,            # Learning rate yang umum untuk fine-tuning BERT\n",
        "    warmup_steps=500,              # Pemanasan untuk menstabilkan training\n",
        "    weight_decay=0.01,             # Regularisasi untuk mencegah overfitting\n",
        "\n",
        "    # --- Optimasi Performa ---\n",
        "    fp16=True,                     # Aktifkan mixed precision (mempercepat training di GPU T4/V100)\n",
        "    report_to=\"wandb\"              # Jika Anda menggunakan W&B untuk logging\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=eval_dataset,\n",
        "    tokenizer=tokenizer,\n",
        "    compute_metrics=compute_metrics\n",
        ")\n",
        "\n",
        "\n",
        "\n",
        "print(\"\\nMemulai proses fine-tuning model...\")\n",
        "trainer.train()\n",
        "\n",
        "print(\"\\nPelatihan selesai.\")"
      ],
      "metadata": {
        "id": "RAmbQTmuVnKN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# LANGKAH 7: EVALUASI AKHIR\n",
        "## Mengevaluasi model terbaik pada data uji."
      ],
      "metadata": {
        "id": "DwQy7ROXVoId"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\nMengevaluasi model terbaik pada test set...\")\n",
        "evaluation_results = trainer.evaluate()\n",
        "\n",
        "print(\"\\nHasil Evaluasi Akhir:\")\n",
        "print(evaluation_results)\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Pastikan Anda menggunakan test_dataset yang sudah dibuat pada LANGKAH 4\n",
        "print(\"Melakukan prediksi pada test set untuk membuat confusion matrix...\")\n",
        "prediction_output = trainer.predict(test_dataset)\n",
        "\n",
        "# Ambil logits (output mentah model) dan label sebenarnya\n",
        "logits = prediction_output.predictions\n",
        "true_labels = prediction_output.label_ids\n",
        "\n",
        "# Ubah logits menjadi prediksi biner (0 atau 1) menggunakan threshold 0.5\n",
        "# Ini adalah logika yang sama dengan yang ada di fungsi compute_metrics Anda\n",
        "sigmoid = torch.nn.Sigmoid()\n",
        "probs = sigmoid(torch.from_numpy(logits))\n",
        "predictions = (probs > 0.5).int().numpy()\n",
        "\n",
        "# Ambil daftar nama kolom label yang sudah didefinisikan sebelumnya\n",
        "# Misal: label_columns = ['joy', 'trust', 'fear', 'surprise', 'sadness', 'disgust', 'anger', 'anticipation']\n",
        "\n",
        "# Siapkan grid plot untuk menampilkan 8 confusion matrix (2 baris, 4 kolom)\n",
        "fig, axes = plt.subplots(2, 4, figsize=(20, 10))\n",
        "axes = axes.flatten() # Ubah grid 2x4 menjadi array 1D agar mudah di-loop\n",
        "\n",
        "# Loop melalui setiap label untuk membuat confusion matrix-nya\n",
        "for i, label_name in enumerate(label_columns):\n",
        "    # Hitung confusion matrix untuk label ke-i\n",
        "    cm = confusion_matrix(\n",
        "        y_true=true_labels[:, i],  # Label sebenarnya untuk emosi ini\n",
        "        y_pred=predictions[:, i]   # Prediksi model untuk emosi ini\n",
        "    )\n",
        "\n",
        "    # Visualisasikan sebagai heatmap menggunakan Seaborn\n",
        "    ax = axes[i]\n",
        "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=ax,\n",
        "                xticklabels=['Tidak Ada', 'Ada'], yticklabels=['Tidak Ada', 'Ada'])\n",
        "\n",
        "    ax.set_title(f\"Confusion Matrix untuk: {label_name.title()}\", fontweight='bold')\n",
        "    ax.set_xlabel('Prediksi Model')\n",
        "    ax.set_ylabel('Label Sebenarnya')\n",
        "\n",
        "# Atur layout agar rapi dan tidak tumpang tindih\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "g3B6dEEGV0u4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# LANGKAH 8: MENYIMPAN MODEL DAN TOKENIZER FINAL\n",
        "## Menyimpan model yang sudah di-fine-tuning untuk digunakan nanti."
      ],
      "metadata": {
        "id": "pZBET3fPV1zt"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FtHjNPK7T0-M"
      },
      "outputs": [],
      "source": [
        "final_model_path = \"./fine_tuned_indobert_emoji\"\n",
        "print(f\"\\nMenyimpan model dan tokenizer final ke '{final_model_path}'...\")\n",
        "trainer.save_model(final_model_path)\n",
        "tokenizer.save_pretrained(final_model_path)\n",
        "print(\"Model dan tokenizer berhasil disimpan.\")\n",
        "\n",
        "from google.colab import drive\n",
        "import os\n",
        "\n",
        "# Mount Google Drive\n",
        "print(\"Mounting Google Drive...\")\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Tentukan path untuk menyimpan model di Google Drive\n",
        "# Pastikan folder 'training 1' ada di root Google Drive Anda atau sesuaikan path\n",
        "drive_save_path = \"/content/drive/MyDrive/fine_tuned_indobert_emoji/training 1\"\n",
        "\n",
        "# Buat direktori jika belum ada\n",
        "if not os.path.exists(drive_save_path):\n",
        "    os.makedirs(drive_save_path)\n",
        "    print(f\"Direktori '{drive_save_path}' dibuat.\")\n",
        "else:\n",
        "    print(f\"Direktori '{drive_save_path}' sudah ada.\")\n",
        "\n",
        "# Menyimpan model dan tokenizer final ke Google Drive\n",
        "print(f\"\\nMenyimpan model dan tokenizer final ke '{drive_save_path}'...\")\n",
        "trainer.save_model(drive_save_path)\n",
        "tokenizer.save_pretrained(drive_save_path)\n",
        "print(\"Model dan tokenizer berhasil disimpan di Google Drive.\")"
      ]
    }
  ]
}